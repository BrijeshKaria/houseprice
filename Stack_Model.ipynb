{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#load traning data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "import datetime\n",
    "import os.path\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_ONLY_2_Outliers = True\n",
    "USE_DUMMY_CAT_FEATURES = True\n",
    "STACK_MODELLING = True\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = pd.read_csv('./data/train_X_clean.csv')\n",
    "df_X_test =  pd.read_csv('./data/test_X_clean.csv')\n",
    "df_y_train = pd.read_csv('./data/train_y_clean.csv')\n",
    "\n",
    "display(df_X_train.head(1))\n",
    "display(df_X_test.head(1))\n",
    "display(df_y_train.head(1))\n",
    "print(df_X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_X_train.iloc[:, 0:].values \n",
    "print(X.shape)\n",
    "#print(X[0])\n",
    "\n",
    "y = df_y_train.iloc[:, 0:].values \n",
    "print(y.shape)\n",
    "#print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "kfolds = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# rmsle\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "# build our model scoring function\n",
    "def cv_rmse(model, X=X):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)\n",
    "\n",
    "# setup models    \n",
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n",
    "\n",
    "xgboost = xgb.XGBRegressor(colsample_bytree=0.4, gamma=0,learning_rate=0.03, max_depth=3, min_child_weight=1.5,\n",
    "                 n_estimators=10000, reg_alpha=0.75, reg_lambda=0.45,subsample=0.6, seed=42) \n",
    "\n",
    "\n",
    "ridge = make_pipeline(RobustScaler(),\n",
    "                      RidgeCV(alphas=alphas_alt, cv=kfolds,))\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(),\n",
    "                      LassoCV(max_iter=1e7, alphas=alphas2,\n",
    "                              random_state=42, cv=kfolds))\n",
    "\n",
    "elasticnet = make_pipeline(RobustScaler(),\n",
    "                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n",
    "                                        cv=kfolds, random_state=42, l1_ratio=e_l1ratio))\n",
    "                                        \n",
    "svr = make_pipeline(RobustScaler(),\n",
    "                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "svr = SVR(kernel = 'rbf',gamma='auto')\n",
    "gbr = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt',\n",
    "                                min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)\n",
    "lightgbm = LGBMRegressor(objective='regression', num_leaves=4,learning_rate=0.01, n_estimators=5000,\n",
    "                         max_bin=200, bagging_fraction=0.75,bagging_freq=5, bagging_seed=7,feature_fraction=0.2,\n",
    "                         feature_fraction_seed=7,verbose=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stack_gen = StackingCVRegressor(regressors=(ridge, lasso, svr, lightgbm, gbr, xgboost, rf),\n",
    "                                meta_regressor=xgboost,use_features_in_secondary=True)\n",
    "\n",
    "print('TEST score on CV')\n",
    "\n",
    "score = cv_rmse(ridge)\n",
    "print(\"\\nKernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(svr)\n",
    "print(\"\\nSVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(lightgbm)\n",
    "print(\"\\nLightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(gbr)\n",
    "print(\"\\nGradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(xgboost)\n",
    "print(\"\\nXgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(rf)\n",
    "print(\"\\nRandomForestRegressor score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "\n",
    "\n",
    "print('START Fit')\n",
    "print(datetime.now(), 'StackingCVRegressor')\n",
    "stack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n",
    "print(datetime.now(), 'lasso')\n",
    "lasso_model_full_data = lasso.fit(X, y)\n",
    "print(datetime.now(), 'ridge')\n",
    "ridge_model_full_data = ridge.fit(X, y)\n",
    "print(datetime.now(), 'svr')\n",
    "svr_model_full_data = svr.fit(X, y)\n",
    "print(datetime.now(), 'GradientBoosting')\n",
    "gbr_model_full_data = gbr.fit(X, y)\n",
    "print(datetime.now(), 'xgboost')\n",
    "xgb_model_full_data = xgboost.fit(X, y)\n",
    "print(datetime.now(), 'lightgbm')\n",
    "lgb_model_full_data = lightgbm.fit(X, y)\n",
    "print(datetime.now(), 'RandomForestRegressor')\n",
    "rf_model_full_data = rf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_weights = {}\n",
    "model_weights = {'rf':0.01,'lasso':0.05, 'ridge':0.1, 'svr':0.1, 'gbr':0.2, 'xgb':0.15, 'lgb':0, 'stack':0.3}\n",
    "\n",
    "def blend_models_predict(X_pred):\n",
    "    return ((model_weights['rf'] * rf_model_full_data.predict(X_pred)) + \\\n",
    "            (model_weights['lasso'] * lasso_model_full_data.predict(X_pred)) + \\\n",
    "            (model_weights['ridge'] * ridge_model_full_data.predict(X_pred)) + \\\n",
    "            (model_weights['svr'] * svr_model_full_data.predict(X_pred)) + \\\n",
    "            (model_weights['gbr'] * gbr_model_full_data.predict(X_pred)) + \\\n",
    "            (model_weights['xgb'] * xgb_model_full_data.predict(X_pred)) + \\\n",
    "            (model_weights['lgb'] * lgb_model_full_data.predict(X_pred)) + \\\n",
    "            (model_weights['stack'] * stack_gen_model.predict(np.array(X_pred))))\n",
    "\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "#print(y.shape)\n",
    "#print(rmsle(y, blend_models_predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_X_test.iloc[:, 0:].values \n",
    "print(X_test.shape)\n",
    "#print(X_test[0:])\n",
    "print('Predict submission', datetime.now(),)\n",
    "submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "print(submission.columns)\n",
    "submission.drop('SalePrice',axis=1)\n",
    "print(\"Shape of submission is {0}\".format(submission.shape))\n",
    "print(submission.columns)\n",
    "#submission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_test)))\n",
    "#y_test = np.floor(np.expm1(blend_models_predict(X_test[0])))\n",
    "y_test = np.floor(np.expm1(xgb_model_full_data.predict(X_test)))\n",
    "\n",
    "print(\"Shape of output is {0}\".format(y_test.shape))\n",
    "print(y_test)\n",
    "dataset = pd.DataFrame({'SalePrice': y_test})\n",
    "\n",
    "print(y_test[0:5])\n",
    "print(len(y_test[0:2]))\n",
    "\n",
    "\n",
    "result = pd.concat([submission['Id'], dataset], axis=1, sort=False)\n",
    "print(result.head())\n",
    "result.to_csv(\"submission.csv\", index=False)\n",
    "print('Save submission', datetime.now(),)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
